{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f488187-0593-48d6-8ad3-7b5bdb0f607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geneformer\n",
    "from geneformer import TranscriptomeTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import loompy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c2b851-cf1e-4685-8450-f989701709a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to redo tokenization while permutating the data to see evaluate geneformer model\n",
    "# Tk will tokenize all loom files in a directory so make sure each loom file as its own directory\n",
    "data_path = \"/u/home/r/ramadas/geneformer_tokenized_data\"\n",
    "output_path = \"/u/home/r/ramadas/geneformer_tokenized_data/permuted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474b079-228f-478d-980d-c16c376111db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import loompy\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Org file \n",
    "original_loom = \"/u/home/r/ramadas/geneformer_tokenized_data/labeled_t_cell_data.loom\"\n",
    "# Copy so we can permute it\n",
    "permuted_loom = \"/u/home/r/ramadas/geneformer_tokenized_data/labeled_t_cell_data_permuted.loom\"\n",
    "\n",
    "# Remove the file if it already exists\n",
    "if os.path.exists(permuted_loom):\n",
    "    os.remove(permuted_loom)\n",
    "    \n",
    "with loompy.new(permuted_loom) as ds_permute:\n",
    "    with loompy.connect(original_loom) as ds:\n",
    "        ds_permute.add_columns(ds.layers, col_attrs=ds.col_attrs, row_attrs=ds.row_attrs)\n",
    "\n",
    "        original_stim = list(ds.ca['stimulation'])\n",
    "        print(\"Original stimulation distribution:\", Counter(original_stim))\n",
    "\n",
    "        print(\"First 10 original values:\", original_stim[:10])\n",
    "        \n",
    "        # Create a copy and shuffle it\n",
    "        permuted_stim = np.array(original_stim, dtype=np.str_).copy()\n",
    "        np.random.shuffle(permuted_stim)  # Shuffles in-place\n",
    "\n",
    "        print(\"First 10 shuffled values:\", permuted_stim[:10])\n",
    "\n",
    "        same_position = np.mean(original_stim == permuted_stim) * 100\n",
    "        print(f\"Percentage of values in same position: {same_position:.2f}%\")\n",
    "        \n",
    "        # Replace the stimulation data in the new file\n",
    "        ds_permute.ca['stimulation'] = permuted_stim\n",
    "        print(\"Updated permuted file successfully\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf23b27-422e-417e-9b2f-41833b54c0a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing /u/home/r/ramadas/geneformer_tokenized_data/labeled_t_cell_data.loom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 94/94 [01:33<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/u/home/r/ramadas/geneformer_tokenized_data/labeled_t_cell_data__dedup.loom has no column attribute 'filter_pass'; tokenizing all cells.\n",
      "Tokenizing /u/home/r/ramadas/geneformer_tokenized_data/labeled_t_cell_data__dedup.loom\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "File '/u/home/r/ramadas/geneformer_tokenized_data/labeled_t_cell_data__dedup.loom' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Geneformer created new models so we have to specify \u001b[39;00m\n\u001b[1;32m      2\u001b[0m tk \u001b[38;5;241m=\u001b[39m TranscriptomeTokenizer({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstimulation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstimulation\u001b[39m\u001b[38;5;124m'\u001b[39m}, nproc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, model_input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQ_data_permuted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/geneformer/tokenizer.py:425\u001b[0m, in \u001b[0;36mTranscriptomeTokenizer.tokenize_data\u001b[0;34m(self, data_directory, output_directory, output_prefix, file_format, use_generator)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_data\u001b[39m(\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    402\u001b[0m     data_directory: Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m     use_generator: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    407\u001b[0m ):\n\u001b[1;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m    Tokenize .loom files in data_directory and save as tokenized .dataset in output_directory.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m \n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     tokenized_cells, cell_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_format\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m     tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_dataset(\n\u001b[1;32m    429\u001b[0m         tokenized_cells,\n\u001b[1;32m    430\u001b[0m         cell_metadata,\n\u001b[1;32m    431\u001b[0m         use_generator\u001b[38;5;241m=\u001b[39muse_generator,\n\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    434\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m (Path(output_directory) \u001b[38;5;241m/\u001b[39m output_prefix)\u001b[38;5;241m.\u001b[39mwith_suffix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/geneformer/tokenizer.py:456\u001b[0m, in \u001b[0;36mTranscriptomeTokenizer.tokenize_files\u001b[0;34m(self, data_directory, file_format)\u001b[0m\n\u001b[1;32m    454\u001b[0m file_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 456\u001b[0m file_tokenized_cells, file_cell_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_file_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m tokenized_cells \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m file_tokenized_cells\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_attr_name_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/geneformer/tokenizer.py:551\u001b[0m, in \u001b[0;36mTranscriptomeTokenizer.tokenize_loom\u001b[0;34m(self, loom_file_path, target_sum)\u001b[0m\n\u001b[1;32m    548\u001b[0m loom_file_path_original \u001b[38;5;241m=\u001b[39m loom_file_path\n\u001b[1;32m    550\u001b[0m dedup_filename \u001b[38;5;241m=\u001b[39m loom_file_path\u001b[38;5;241m.\u001b[39mwith_name(loom_file_path\u001b[38;5;241m.\u001b[39mstem \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dedup.loom\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 551\u001b[0m loom_file_path \u001b[38;5;241m=\u001b[39m \u001b[43msum_ensembl_ids\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloom_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollapse_gene_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgene_mapping_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgene_token_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_attr_name_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m lp\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;28mstr\u001b[39m(loom_file_path)) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;66;03m# define coordinates of detected protein-coding or miRNA genes and vector of their normalization factors\u001b[39;00m\n\u001b[1;32m    563\u001b[0m     coding_miRNA_loc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m    564\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenelist_dict\u001b[38;5;241m.\u001b[39mget(i, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mra[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensembl_id_collapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    565\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/geneformer/tokenizer.py:99\u001b[0m, in \u001b[0;36msum_ensembl_ids\u001b[0;34m(data_directory, collapse_gene_ids, gene_mapping_dict, gene_token_dict, custom_attr_name_dict, file_format, chunk_size)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    Map Ensembl IDs from gene mapping dictionary. If duplicate Ensembl IDs are found, sum counts together.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensembl_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mra\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    102\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensembl_id\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column missing from data.ra.keys()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensembl_id_collapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mra\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    106\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensembl_id_collapsed\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column already exists in data.ra.keys()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/loompy/loompy.py:1634\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(filename, mode, validate, spec_version)\u001b[0m\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m, validate: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, spec_version: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LoomConnection:\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;250m\t\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;124;03m\tEstablish a connection to a .loom file.\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;124;03m\t\tNote: if validation is requested, an exception is raised if validation fails.\u001b[39;00m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1634\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLoomConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/loompy/loompy.py:76\u001b[0m, in \u001b[0;36mLoomConnection.__init__\u001b[0;34m(self, filename, mode, validate)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mEstablish a connection to a Loom file.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\tNothing.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filename):\n\u001b[0;32m---> 76\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# make sure a valid mode was passed\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: File '/u/home/r/ramadas/geneformer_tokenized_data/labeled_t_cell_data__dedup.loom' not found"
     ]
    }
   ],
   "source": [
    "# Geneformer created new models so we have to specify \n",
    "tk = TranscriptomeTokenizer({'stimulation': 'stimulation'}, nproc=16, model_input_size = 2048)\n",
    "tk.tokenize_data(data_path, output_path, \"Q_data_permuted\", file_format=\"loom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fce04a-82eb-4695-a297-b9485d0d5b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1_env",
   "language": "python",
   "name": "test1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
